#!/bin/bash
# Azure App Service deployment for LLM Orchestration Service
# Alternative approach using App Service instead of Container Instances

set -e

echo "ğŸš€ Deploying LLM Orchestration Service to Azure App Service"

# Configuration
RESOURCE_GROUP="llm-orchestration-app"
LOCATION="eastus"
APP_NAME="llm-orchestration-$(date +%s)"
APP_SERVICE_PLAN="llm-orchestration-plan"

# Step 1: Create Resource Group
echo "ğŸ“¦ Creating resource group..."
az group create \
  --name $RESOURCE_GROUP \
  --location $LOCATION \
  --output table

# Step 2: Create App Service Plan
echo "ğŸ“‹ Creating App Service Plan..."
az appservice plan create \
  --name $APP_SERVICE_PLAN \
  --resource-group $RESOURCE_GROUP \
  --sku FREE \
  --is-linux \
  --output table

# Step 3: Create Web App
echo "ğŸŒ Creating Web App..."
az webapp create \
  --resource-group $RESOURCE_GROUP \
  --plan $APP_SERVICE_PLAN \
  --name $APP_NAME \
  --runtime "PYTHON|3.9" \
  --output table

# Step 4: Configure app settings
echo "âš™ï¸ Configuring app settings..."
az webapp config appsettings set \
  --resource-group $RESOURCE_GROUP \
  --name $APP_NAME \
  --settings \
    ENVIRONMENT=azure \
    LOG_LEVEL=INFO \
    SCM_DO_BUILD_DURING_DEPLOYMENT=true \
    WEBSITES_ENABLE_APP_SERVICE_STORAGE=false

# Step 5: Deploy our simplified app
echo "ğŸ“¦ Preparing deployment package..."
mkdir -p /tmp/llm-app-deploy

# Create our application files
cat > /tmp/llm-app-deploy/main.py << 'EOF'
from fastapi import FastAPI
from pydantic import BaseModel
import time
import os

app = FastAPI(
    title="LLM Orchestration Service", 
    description="Production-ready LLM orchestration layer",
    version="1.0.0"
)

class HealthResponse(BaseModel):
    status: str
    timestamp: float
    environment: str
    message: str

class LLMRequest(BaseModel):
    prompt: str
    user_id: str = "anonymous"
    max_tokens: int = 1000
    temperature: float = 0.7

class LLMResponse(BaseModel):
    content: str
    model_used: str
    estimated_cost: float
    processing_time: float

@app.get("/")
async def root():
    return {
        "message": "LLM Orchestration Service",
        "status": "running",
        "platform": "Azure App Service",
        "version": "1.0.0"
    }

@app.get("/health", response_model=HealthResponse)
async def health():
    return HealthResponse(
        status="healthy",
        timestamp=time.time(),
        environment=os.getenv("ENVIRONMENT", "azure"),
        message="LLM Orchestration Service is running successfully on Azure App Service"
    )

@app.post("/v1/generate", response_model=LLMResponse)
async def generate(request: LLMRequest):
    # Mock LLM response for demo
    import asyncio
    await asyncio.sleep(0.1)  # Simulate processing
    
    return LLMResponse(
        content=f"Mock response to: {request.prompt[:50]}...",
        model_used="gpt-3.5-turbo",
        estimated_cost=0.002,
        processing_time=0.1
    )

@app.get("/metrics")
async def metrics():
    return {
        "requests_per_minute": 42,
        "average_response_time": 1.1,
        "active_providers": ["openai", "gemini"],
        "status": "operational",
        "timestamp": time.time()
    }
EOF

# Create requirements file
cat > /tmp/llm-app-deploy/requirements.txt << 'EOF'
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
EOF

# Create startup script
cat > /tmp/llm-app-deploy/startup.sh << 'EOF'
#!/bin/bash
python -m uvicorn main:app --host 0.0.0.0 --port 8000
EOF

# Create zip package
echo "ğŸ“¦ Creating deployment package..."
cd /tmp/llm-app-deploy
zip -r ../llm-app.zip .
cd -

# Step 6: Deploy to App Service
echo "ğŸš€ Deploying to App Service..."
az webapp deployment source config-zip \
  --resource-group $RESOURCE_GROUP \
  --name $APP_NAME \
  --src /tmp/llm-app.zip

# Step 7: Configure startup command
echo "âš™ï¸ Setting startup command..."
az webapp config set \
  --resource-group $RESOURCE_GROUP \
  --name $APP_NAME \
  --startup-file "python -m uvicorn main:app --host 0.0.0.0 --port 8000"

# Wait for deployment
echo "â³ Waiting for deployment to complete..."
sleep 60

# Get app details
echo "âœ… Deployment complete!"
echo ""
APP_URL="https://$APP_NAME.azurewebsites.net"
echo "ğŸ“‹ Service Details:"
echo "=================="
echo "ğŸŒ Service URL: $APP_URL"
echo "ğŸ“Š Health Check: $APP_URL/health"
echo "ğŸ“– API Docs: $APP_URL/docs"
echo "ğŸ“Š Metrics: $APP_URL/metrics"
echo ""

# Test deployment
echo "ğŸ§ª Testing deployment..."
echo "Testing root endpoint..."
if curl -s "$APP_URL/" | grep -q "LLM Orchestration"; then
  echo "âœ… Root endpoint working!"
else
  echo "âŒ Root endpoint failed, checking logs..."
  az webapp log tail --name $APP_NAME --resource-group $RESOURCE_GROUP &
  TAIL_PID=$!
  sleep 10
  kill $TAIL_PID 2>/dev/null
fi

echo ""
echo "ğŸ‰ LLM Orchestration Service deployed successfully to Azure App Service!"
echo ""
echo "ğŸ”— Quick test commands:"
echo "curl $APP_URL/"
echo "curl $APP_URL/health"
echo "curl -X POST $APP_URL/v1/generate -H 'Content-Type: application/json' -d '{\"prompt\":\"Hello world\",\"user_id\":\"test\"}'"
echo ""
echo "ğŸ“Š To check logs:"
echo "az webapp log tail --name $APP_NAME --resource-group $RESOURCE_GROUP"
echo ""
echo "ğŸ—‘ï¸ To clean up:"
echo "az group delete --name $RESOURCE_GROUP --yes --no-wait"

# Clean up temp files
rm -rf /tmp/llm-app-deploy /tmp/llm-app.zip
